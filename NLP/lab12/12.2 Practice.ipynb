{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 20-Newsgroup Dataset Classification with GloVe and Keras\n",
    "TA: Bokyung Son (*Computational Linguistics Lab*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Specification\n",
    "This dataset is a collection of 20,000 messages, collected from 20 different netnews newsgroups. 1,000 messages from each of the 20 newsgroups were chosen at random and partitioned by newsgroup name. The list of newsgroups is as follows:\n",
    "\n",
    "- alt.atheism\n",
    "- talk.politics.guns\n",
    "- talk.politics.mideast\n",
    "- talk.politics.misc\n",
    "- talk.religion.misc\n",
    "- soc.religion.christian\n",
    "- comp.sys.ibm.pc.hardware\n",
    "- comp.graphics\n",
    "- comp.os.ms-windows.misc\n",
    "- comp.sys.mac.hardware\n",
    "- comp.windows.x\n",
    "- rec.autos\n",
    "- rec.motorcycles\n",
    "- rec.sport.baseball\n",
    "- rec.sport.hockey\n",
    "- sci.crypt\n",
    "- sci.electronics\n",
    "- sci.space\n",
    "- sci.med\n",
    "- misc.forsale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Windows users,\n",
    "\n",
    "[Download file](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz)\n",
    "\n",
    "[Download 7-zip](https://www.7-zip.org/) and use it to unzip the tar.gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MAC/linux users,\n",
    "!wget http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n",
    "!tar -xvzf news20.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained GloVe\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir) \n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Dense, Input, Bidirectional, LSTM\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOVE_DIR = './glove.6B'\n",
    "GLOVE_DIR = '.'\n",
    "TEXT_DATA_DIR = './20_newsgroup'\n",
    "MAX_SEQUENCE_LENGTH = 1000 # padding전에 sequence 맞추기\n",
    "MAX_FEATURES = 20000 # 몇개의 most common 단어를 볼것인가?\n",
    "EMBEDDING_DIM = 100 # embedding하는 dimension 을 선택\n",
    "VALIDATION_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-90a385e801e9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-90a385e801e9>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    - 로이터는 이미 2000개정도의 word의 indexing 해서 들어왔다. (즉, 가공된 데이터셋)\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "- 로이터는 이미 2000개정도의 word의 indexing 해서 들어왔다. (즉, 가공된 데이터셋)\n",
    "- 실제 텍스트를 할떄는 그 작업을 직접 해야한다.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vectorize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "# MAP: word -> embedding vector in embeddings set\n",
    "# 뉴스기사의 word를 pretrain된 glove의 vector로 mapping 해준다\n",
    "fileObj = codecs.open( \"glove.6B.100d.txt\", \"r\", \"utf-8\" )\n",
    "u = fileObj.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for line in u:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_index['apple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare text samples and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset...\n"
     ]
    }
   ],
   "source": [
    "print('Processing text dataset...')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Found %s texts.' % len(texts))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocessing\n",
    "See [Tokenizer documentation](https://keras.io/preprocessing/text/#tokenizer) and its [older version](https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/#tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer Class vectorizes and turns text into list of word indices(ranks)\n",
    "# fit_on_texts : \n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(texts)  # train on `texts (list of sample texts)`\n",
    "sequences = tokenizer.texts_to_sequences(texts)  # return list of sequences (one per text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1237, 273, 1213, 1439, 1071, 1213, 1237, 273, 1439, 192, 2515, 348, 2964, 779, 332, 28, 45, 1628, 1439, 2516, 3, 1628, 2144, 780, 937, 29, 441, 2770, 8854, 4601, 7969, 11979, 5, 12806, 75, 1628, 19, 229, 29, 1, 937, 29, 441, 2770, 6, 1, 118, 558, 2, 90, 106, 482, 3979, 6602, 5375, 1871, 12260, 1632, 17687, 1828, 5101, 1828, 5101, 788, 1, 8854, 4601, 96, 4, 4601, 5455, 64, 1, 751, 563, 1716, 15, 71, 844, 24, 20, 1971, 5, 1, 389, 8854, 744, 1023, 1, 7762, 1300, 2912, 4601, 8, 73, 1698, 6, 1, 118, 558, 2, 1828, 5101, 16500, 13447, 73, 1261, 10982, 170, 66, 6, 1, 869, 2235, 2544, 534, 34, 79, 8854, 4601, 29, 6603, 3388, 264, 1505, 535, 49, 12, 343, 66, 60, 155, 2, 6603, 1043, 1, 427, 8, 73, 1698, 618, 4601, 417, 1628, 632, 11716, 4602, 814, 1628, 691, 3, 1, 467, 2163, 3, 2266, 7491, 5, 48, 15, 40, 135, 378, 8, 1, 467, 6359, 30, 101, 90, 1781, 5, 115, 101, 417, 1628, 632, 17061, 1448, 4317, 45, 860, 73, 1611, 2455, 3343, 467, 7491, 13132, 5814, 1301, 1781, 1, 467, 9477, 667, 11716, 323, 15, 1, 1074, 802, 332, 3, 1, 467, 558, 2, 417, 1628, 632, 90, 106, 482, 2030, 2408, 22, 13799, 853, 2030, 2408, 1871, 3793, 12524, 439, 3793, 13448, 691, 788, 691, 502, 1552, 11221, 116, 993, 558, 2, 2974, 996, 7674, 1184, 1346, 108, 828, 1871, 9478, 12807, 32, 7675, 460, 61, 110, 16, 3362, 22, 1950, 8, 691, 1711, 5622, 233, 1346, 1428, 4623, 1260, 12, 16501, 32, 1044, 7854, 564, 3955, 16501, 5, 1, 500, 3, 564, 27, 4602, 4, 9648, 2913, 10746, 558, 2, 7128, 97, 2456, 2420, 4623, 1260, 12, 16501, 90, 106, 482, 13133, 1346, 1428, 797, 2652, 632, 2366, 445, 3955, 681, 2477, 288, 1184, 15965, 853, 2797, 7308, 2797, 15438, 6902, 15438, 10747, 1538, 15000, 2366, 1380, 337, 5994, 681, 338, 13800, 3138, 1995, 2797, 728, 12808, 3558, 15438, 11717, 2797, 439, 15438, 11717, 15438, 1, 445, 3955, 681, 4602, 1, 4, 5254, 1689, 4712, 6, 1282, 103, 152, 6421, 9020, 19947, 19120, 141, 2230, 4545, 1574, 1282, 4602, 4, 2341, 9020, 2341, 6421, 9020, 103, 152, 19947, 19120, 141, 2230, 4545, 1574, 1282, 12, 1628, 691, 558, 2, 129, 6421, 19947, 141, 3857, 19121, 28, 1282, 1871, 691, 4429, 1058, 56, 1, 3542, 17062, 3316, 723, 889, 1, 4430, 1333, 9, 3542, 959, 36, 3158, 5, 1143, 19, 55, 13134, 2, 980, 22, 628, 2224, 2267, 107, 4624, 56, 2280, 2420, 4, 12, 40, 13135, 6, 14, 301, 4296, 8205, 8, 1, 60, 1930, 71, 1133, 6484, 29, 4984, 8323, 1, 6675, 3, 1063, 20, 5815, 5, 2785, 594, 701, 5, 2557, 8855, 301, 4296, 8205, 261, 6, 478, 1, 514, 12, 333, 18371, 9, 147, 1717, 2601, 22, 94, 55, 4124, 2590, 3881, 8206, 171, 3858, 8206, 171, 3858, 3858, 355, 125, 6307, 5, 359, 19122, 723, 2309, 5, 85, 2309, 19, 7763, 31, 387, 24, 117, 51, 355, 3120, 2701, 24, 51, 355, 44, 66, 511, 5, 441, 298, 82, 547, 536, 51, 491, 1858, 9, 51, 81, 2526, 58, 576, 3, 137, 51, 5922, 13449, 5870, 85, 1, 347, 19, 3, 58, 7855, 10348, 6676, 4, 5923, 5871, 4, 294, 3, 544, 5, 714, 2, 4, 1585, 2001, 2, 2409, 4, 6308, 29, 11478, 1, 67, 1, 5871, 4087, 2, 2310, 577, 29, 1, 6676, 1144, 8, 2786, 2, 12525, 4, 8856, 5, 8587, 8205, 4, 3, 571, 17063, 12, 149, 1758, 3, 4, 547, 323, 441, 1, 9182, 6677, 12, 1, 3749, 10008, 3, 972, 154, 1172, 8, 2602, 119, 85, 1725, 30, 4, 5456, 2738, 5338, 3, 2678, 24, 1370, 3077, 2478, 51, 8, 10983, 30, 85, 15439, 5, 1628, 916, 5, 12806, 75, 3329, 3158, 1, 3077, 4271, 137, 544, 30, 531, 4, 886, 1623, 6977, 21, 280, 4498, 29, 209, 1786, 140, 1140, 280, 8, 4741, 5, 200, 16, 9649, 30, 4, 628, 313, 1106, 1725, 8, 8207, 2, 408, 2200, 715, 3406, 2121, 19948, 1, 8324, 4, 889, 323, 15, 1, 4431, 9, 1, 118, 1166, 8, 5, 9479, 1939, 179, 1535, 3, 1, 1718, 2, 261, 11, 132, 248, 1, 378, 8, 1, 17688, 3, 4, 11718, 275, 21, 280, 4339, 2, 487, 223, 1, 108, 422, 6550, 132, 2, 197, 1686, 8, 5, 71, 1904, 3058, 19, 1413, 5995, 19, 14183, 5, 1, 784, 8, 76, 151, 12, 10178, 29, 1, 467, 2679, 19, 5763, 2345, 60, 3139, 845, 4625, 6, 1, 309, 210, 19, 220, 5, 11719, 1349, 1922, 8, 1120, 2, 79, 151, 2, 31, 126, 24, 1, 8324, 9330, 68, 5, 68, 16502, 21, 11, 654, 15, 814, 3187, 1, 467, 14, 2009, 8720, 5, 19123, 156, 52, 491, 83, 15966, 240, 11, 8, 255, 811, 655, 23, 76, 48, 9, 1008, 74, 35, 36, 1, 16503, 8, 44, 11, 959, 6, 125, 251, 1680, 48, 122, 203, 13, 79, 1, 40, 254, 332, 691, 314, 4429, 870, 415, 13136, 3, 582, 632, 1649, 536, 415, 13136, 253, 2, 16, 422, 22, 114, 1787, 14, 8, 4, 117, 500, 3, 13450, 17689, 207, 1839, 2297, 995, 1165, 2268, 549, 2330, 1213, 4, 6307, 3559, 3078, 219, 632, 2527, 780, 4, 3215, 5, 7856, 3559, 3, 1213, 1301, 32, 10179, 6756, 7970, 5, 2753, 6, 14, 1341, 534, 11479, 194, 12, 3170, 1213, 7, 103, 1, 314, 973, 6, 1, 970, 3, 137, 87, 5, 86, 12, 1840, 1213, 1, 973, 6, 1, 314, 970, 3, 137, 87, 838, 272, 3, 1, 134, 12809, 1403, 12, 137, 699, 1796, 8, 1476, 2, 4432, 135, 21, 5, 1506, 4317, 45, 14581, 47, 15001, 9826, 86, 229, 1, 238, 245, 972, 3078, 219, 632, 4, 3859, 12526, 3, 972, 6, 61, 51, 6604, 1, 259, 7764, 3, 972, 5, 5171, 7309, 9, 27, 19, 5, 22, 18372, 8857, 1506, 4317, 45, 19124, 78, 802, 6309, 204, 137, 204, 7129, 1, 13451, 6116, 219, 632, 3059, 2113, 780, 1, 4398, 3, 6, 1174, 1, 120, 6, 61, 363, 8858, 22, 15002, 1774, 4, 5558, 1862, 210, 600, 15, 1, 707, 9183, 5, 196, 1502, 2914, 5, 4499, 1, 4375, 8, 15, 417, 5, 1414, 108, 2894, 10566, 1064, 4, 591, 500, 3, 22, 1213, 204, 137, 204, 7129, 8, 298, 1, 4376, 500, 3, 1, 5299, 3, 4, 553, 379, 1, 973, 9, 137, 959, 9331, 1506, 4317, 15001, 45, 59, 1063, 45, 73, 1145, 2068, 1, 272, 2503, 691, 108, 828, 780, 4, 2727, 3, 9827, 3, 4, 251, 392, 15, 1494, 5, 4143, 61, 4009, 22, 11480, 839, 1, 6978, 3060, 5, 210, 600, 838, 6605, 5, 491, 10748, 420, 29, 125, 66, 12, 58, 1600, 2399, 9184, 1, 120, 6, 61, 814, 66, 1699, 5, 8208, 1, 379, 153, 1, 3956, 346, 4, 212, 3, 1, 9827, 19, 4144, 29, 35, 272, 537, 84, 3, 441, 5, 1323, 3, 441, 17064, 1506, 4317, 1063, 45, 9021, 59, 1247, 1, 970, 3, 137, 5623, 2455, 5255, 14, 378, 8, 1, 295, 1227, 6, 4, 9, 2010, 20, 1, 3, 3543, 4868, 5, 25, 5924, 20, 577, 5, 310, 4985, 6, 14, 156, 2540, 2, 8588, 4, 772, 3, 12527, 1403, 12, 1, 970, 3, 137, 85, 1403, 61, 19, 2009, 5, 3771, 675, 1, 3, 1185, 6757, 1719, 1209, 422, 805, 5, 2, 4, 137, 61, 8, 2754, 21, 633, 21, 34, 16, 9828, 70, 4241, 6, 1, 5300, 3, 3543, 6, 1, 5623, 2455, 3, 1, 970, 3, 137, 838, 32, 6, 61, 51, 402, 4, 2009, 18372, 1186, 2, 13137, 183, 162, 13137, 1, 5300, 3, 3543, 5255, 14, 1227, 1301, 4, 3859, 1681, 3, 1, 7217, 1403, 12, 5, 245, 1, 970, 3, 137, 11, 8325, 29, 1, 7587, 6307, 2820, 3, 10180, 1068, 1782, 813, 213, 1, 656, 1403, 3, 11720, 5, 2, 1, 1175, 3, 1, 7587, 30, 5, 11, 86, 2516, 127, 2820, 61, 2613, 1, 1615, 3, 137, 1342, 1, 7971, 3, 1, 2771, 135, 21, 127, 3, 5, 5376, 21, 107, 21, 10984, 12, 137, 135, 21, 1, 378, 8, 4, 19949, 2, 205, 364, 5, 184, 744, 82, 473, 5, 1468, 67, 1651, 20, 1, 501, 8859, 3, 802, 4, 1552, 11221, 32, 12810, 500, 3, 591, 1214, 5, 12528, 691, 882, 31, 591, 4835, 29, 2575, 387, 2, 1, 839, 236, 5, 17, 76, 30, 563, 1149, 3, 1166, 4035, 344, 212, 3036, 1408, 7128, 97, 2456, 2420, 4623, 417, 16501, 32, 116, 1, 4603, 12, 4623, 1260, 12, 16501, 319, 2346, 5872, 32, 3, 1213, 5, 691, 32, 5256, 4, 1463, 1259, 3, 4805, 502, 4869, 4569, 1242, 5, 5, 4869, 500, 3, 3859, 7676, 141, 9480, 1, 505, 3, 1, 467, 5713, 691, 4, 823, 3, 121, 66, 670, 422, 9479, 5, 35, 745, 11, 52, 15, 77, 343, 1439, 452, 4, 436, 176, 323, 1237, 546, 31, 2932, 552, 324, 61, 5506, 3171, 3, 309, 1071, 1213, 9650, 1415, 5, 12806, 75, 353, 12, 68, 185, 351, 176, 2, 1237, 546, 2932, 552, 324, 409, 214, 351, 1213, 2632, 5, 11, 41, 176, 173, 4, 930, 2050]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([18448,  1067,   222,    82,    42,    27,    10,    18,     6,\n",
       "            6,     5,     7,     6,     7,    11,     7,     2,     6,\n",
       "            1,     3,     3,     0,     1,     1,     2,     1,     2,\n",
       "            3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     1], dtype=int64),\n",
       " array([    0.  ,   548.84,  1097.68,  1646.52,  2195.36,  2744.2 ,\n",
       "         3293.04,  3841.88,  4390.72,  4939.56,  5488.4 ,  6037.24,\n",
       "         6586.08,  7134.92,  7683.76,  8232.6 ,  8781.44,  9330.28,\n",
       "         9879.12, 10427.96, 10976.8 , 11525.64, 12074.48, 12623.32,\n",
       "        13172.16, 13721.  , 14269.84, 14818.68, 15367.52, 15916.36,\n",
       "        16465.2 , 17014.04, 17562.88, 18111.72, 18660.56, 19209.4 ,\n",
       "        19758.24, 20307.08, 20855.92, 21404.76, 21953.6 , 22502.44,\n",
       "        23051.28, 23600.12, 24148.96, 24697.8 , 25246.64, 25795.48,\n",
       "        26344.32, 26893.16, 27442.  ]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This helps determine MAX_SEQUENCE_LENGTH\n",
    "# 전체 sqeuence의 lenth 분포를 보고 자르자 pad 값을\n",
    "seqlen = np.array([len(sequence) for sequence in sequences])\n",
    "np.histogram(seqlen, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174074 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Map: word -> rank/index(int) in text\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Pad\n",
    "# 대부분이 1000이하인데 길이가 20000으로하면 낭비잖아요? 그래서~ 간단한게 한게 seqlen 에서 찾자\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 20)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize labels into one-hot\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 19994 19995 19996]\n"
     ]
    }
   ],
   "source": [
    "# split train/validation/test data\n",
    "\n",
    "# required: manually shuffle the data.\n",
    "# by default, validation_split in `fit` does not shuffle the data\n",
    "indices = np.arange(data.shape[0])\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13999, 1000) (2999, 1000) (2999, 1000)\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "num_test_samples = int(TEST_SPLIT * data.shape[0])\n",
    "\n",
    "# first portion goes for validation\n",
    "x_val = data[:num_validation_samples]\n",
    "y_val = labels[:num_validation_samples]\n",
    "\n",
    "# last portion goes for test\n",
    "x_test = data[-num_test_samples:]\n",
    "y_test = labels[-num_test_samples:]\n",
    "\n",
    "# remaining for train\n",
    "x_train = data[num_validation_samples:-num_test_samples]\n",
    "y_train = labels[num_validation_samples:-num_test_samples]\n",
    "\n",
    "print(x_train.shape, x_val.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     6  1411  8981  1990  4650 14524  8981    26    43    13    19  1469\n",
      "   611  2492  1269    19   290    24     4  1595     3   611    57    23\n",
      "     7   112   308    24    73    22    78   198   449    13   252   375\n",
      "     4  2777     2   116     1  2492   464     7    38  2056    33   341\n",
      "     9   664     3     1    48   291  2492  1269   980   806     3  2049\n",
      "    31     1   839    93    34    17   273    68    82    47   757    29\n",
      "    71  1649   752    15     1    75   501   434    55  3213  1159   980\n",
      "  1894     6     1   797   478     2   273    47   757    29    55   372\n",
      "   195     5   664     3    77   255   105    13   105    16   132    91\n",
      "   248    13   105    16   308    14   476     8   960     5   302 12853\n",
      "   134     3     1  2492  1269    15     1   343   105   273    68    82\n",
      "    47   757    29    71  1649   752    13   105   252   375  4293     2\n",
      "  2492   468   240    33  6661     5     7    70   330     2    79   272\n",
      "  4708    29    12   781   532    61   339   615     1  1886     3     4\n",
      "  1464  2777  2840     1   222     9     4   366  2973  4297    66     2\n",
      "     1  1781  2176    24     9    88    17   292    27     1  1250  7478\n",
      "   366     1    76   310    27    19   817  7478   366     8   111     3\n",
      "  2386  6309     5    85   246   952  1253    61 12332  1252     1  1718\n",
      "    14     8  3474   121    27    70   817  7478   366   114   595    67\n",
      "    27    70   349    13   105   116   134     3     1  2492   468    15\n",
      "  1208    13   105    42     9    12     1  3213    21   107    24     1\n",
      "  2492    81   184   184 12495    35    38    13   298  1895     2  3812\n",
      "  1847     1   269    22  6731     5  3661  1847     5  1293     9  2101\n",
      "   161   124  1268   111     3     1  2492    81     4   271     3  1269\n",
      "   806     3  2049    31   398     6 10721   114   281    46    70   124\n",
      "  2790     2  6235    39    25    21   125  2492  1269    21  5551  1269\n",
      "   114     6  1408    13   105   263  1222     1  2492   464    13    81\n",
      "     2   155     2  6235     2  1222     1  5551     7    34   155  1894\n",
      "     6  1174     5  1222     1  2492   405   559   790  7478   366   177\n",
      "     1  2492   121   302   517     9  2492  1269    19    68  3079  2539\n",
      "    82   790  1269  3035   183  8046  9467   201   525    26   897  6625\n",
      "    11     8   817   109     2   221  2766   595    67    17 16972    20\n",
      "   185   183  4435 10932]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Embedding layer\n",
    "See [Embedding documentation](https://faroit.github.io/keras-docs/1.2.2/layers/embeddings/#embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x1de5860c0b8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index -> pre-trained embedding\n",
    "print('Preparing embedding matrix...')\n",
    "\n",
    "# +1은 unknown word 때문에 했다.\n",
    "num_words = min(MAX_FEATURES, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_FEATURES:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# NOTE: freeze the layer (trainable = False) to prevent the weights from being updated\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "# output: (samples, sequence length, embedding dim)\n",
    "\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where all models are saved\n",
    "BASE_PATH = './news20_model/'\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    os.mkdir(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint(model_name):\n",
    "    # creates a subdirectory under `BASE_PATH`\n",
    "    MODEL_PATH = os.path.join(BASE_PATH, model_name)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.mkdir(MODEL_PATH)\n",
    "    \n",
    "    return ModelCheckpoint(filepath=os.path.join(MODEL_PATH, '{epoch:02d}-{val_loss:.4f}.hdf5'),\n",
    "                           monitor='val_loss',\n",
    "                           verbose=1,\n",
    "                           save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_epochs = 30\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training...')\n",
    "\n",
    "# train a 3-layer bi-LSTM model\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(embedded_sequences)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x = Bidirectional(LSTM(128))(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint = create_checkpoint('lstm')  # checkpoint callback\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=max_epochs,\n",
    "          validation_data=(x_val, y_val),\n",
    "          shuffle=True,\n",
    "          callbacks=[early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test,\n",
    "                           batch_size=batch_size)\n",
    "\n",
    "print(f'----- Evaluation loss and metrics for {len(y_test)} test samples -----')\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
