{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\MinhoLee\\Anaconda3\\envs\\venv\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir) \n",
    "import nltk, re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Dense, Input, Bidirectional, LSTM, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Hannanum\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pickle\n",
    "import codecs\n",
    "import csv\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = './newsData'\n",
    "directory_label = {0: '정치', 1: '경제', 2: '사회', 3 : '생활/문화', 4 : '세계', 5 : '기술/IT', 6 : '연예', 7 : '스포츠'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD_EMBEDDING_WORD2VEC = './wiki.ko/wiki.ko.bin'\n",
    "#WORD_EMBEDDING_WORD2VEC = './wiki.ko/wiki.ko.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "EMBEDDING_DIM = 100\n",
    "#EMBEDDING_DIM = 200\n",
    "#EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.15\n",
    "VALIDATION_SPLIT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konlp Objects\n",
    "komo = Komoran()\n",
    "twitter = Twitter()\n",
    "hannanum = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of texts\n",
    "Texts = []  \n",
    "trainTexts = []\n",
    "validationTexts = []\n",
    "testTexts = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of label ids\n",
    "Labels = []\n",
    "trainLabels = []  \n",
    "validationLabels = []\n",
    "testLabels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary mapping label name to numeric id\n",
    "labels_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence of texts\n",
    "seq = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(Texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load texts data\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[directory_label[int(name)]] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            fpath = os.path.join(path, fname)\n",
    "            args = {'encoding': 'utf-8'}\n",
    "            if(re.findall('.1[6789][0-9]NewsData',fpath)):\n",
    "                # test texts\n",
    "                text = []\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    #pos = twitter.pos(t.strip())\n",
    "                    #pos = hannanum.pos(t.strip())\n",
    "                    pos = komo.pos(t.strip())\n",
    "                    for pair in pos :\n",
    "                        #['NNG', 'NNP', 'VV', 'VA']: part-of-speech of content word\n",
    "                        if (re.findall('^[NV]',pair[1])):\n",
    "                            morpheme = pair[0]+'/'+pair[1]\n",
    "                            text.append(morpheme)\n",
    "                    testTexts.append(text)\n",
    "                testLabels.append(label_id)\n",
    "            else :\n",
    "                # train & validation texts\n",
    "                text = []\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    #pos = twitter.pos(t.strip())\n",
    "                    #pos = hannanum.pos(t.strip())\n",
    "                    pos = komo.pos(t.strip())\n",
    "                    for pair in pos :\n",
    "                        if (re.findall('^[NV]',pair[1])):\n",
    "                            morpheme = pair[0]+'/'+pair[1]\n",
    "                            text.append(morpheme)\n",
    "                    Texts.append(text)\n",
    "                Labels.append(label_id)\n",
    "            seq.append(text)\n",
    "print('Found %s texts.' % len(Texts))\n",
    "print('Found %s texts.' % len(testTexts))\n",
    "print(len(Labels))\n",
    "print(len(testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "'''\n",
    "print(\"mean : \", np.mean(trainLen))\n",
    "print(\"max : \" ,np.max(trainLen))\n",
    "print(\"min : \" ,np.min(trainLen))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_FEATURES = 10000\n",
    "MAX_FEATURES = 20000\n",
    "#MAX_FEATURES = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec\n",
    "model_ko = Word2Vec(seq, min_count=1, size=EMBEDDING_DIM) \n",
    "words = list(model_ko.wv.vocab)\n",
    "embeddings_index = {}\n",
    "for i in words:\n",
    "    embeddings_index[i] = model_ko[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, validation_split in `fit` does not shuffle the data\n",
    "indices = np.arange(len(Texts), dtype=int)\n",
    "print(type(indices[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(Texts))\n",
    "print(indices)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "Texts = np.array(Texts)\n",
    "Labels = np.array(Labels)\n",
    "\n",
    "Texts = Texts[indices]\n",
    "Labels = Labels[indices]\n",
    "\n",
    "indices2 = np.arange(len(testTexts))\n",
    "np.random.shuffle(indices2)\n",
    "\n",
    "testTexts = np.array(testTexts)\n",
    "testLabels = np.array(testLabels)\n",
    "\n",
    "testTexts = testTexts[indices2]\n",
    "testLabels = testLabels[indices2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = int(VALIDATION_SPLIT * len(Texts))\n",
    "\n",
    "validationTexts = Texts[:num_validation_samples]\n",
    "validationLabels = Labels[:num_validation_samples]\n",
    "\n",
    "trainTexts = Texts[num_validation_samples:]\n",
    "trainLabels = Labels[num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainTexts))\n",
    "print(len(validationTexts))\n",
    "print(len(testTexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & validation texts의 길이 분포 list\n",
    "trainLen  = list()\n",
    "for i in range(len(trainTexts)):\n",
    "    trainLen.append(len(trainTexts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & validation texts의 길이 분포 list\n",
    "validationLen  = list()\n",
    "for i in range(len(validationTexts)):\n",
    "    validationLen.append(len(validationTexts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test texts의 길이 분포 list\n",
    "testLen  = list()\n",
    "for i in range(len(testTexts)):\n",
    "    testLen.append(len(testTexts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 text별로 길이 분포\n",
    "sns.distplot(trainLen, hist = True, rug =True);\n",
    "sns.distplot(validationLen, hist = True, rug =True);\n",
    "sns.distplot(testLen, hist= True, rug =True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_SEQUENCE_LENGTH = 150\n",
    "#MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "#MAX_SEQUENCE_LENGTH = 300\n",
    "#MAX_SEQUENCE_LENGTH = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 선언\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "TEXT = list()\n",
    "for i in range(len(trainTexts)):\n",
    "    TEXT.append(trainTexts[i])\n",
    "    \n",
    "for i in range(len(validationTexts)):\n",
    "    TEXT.append(validationTexts[i]) \n",
    "    \n",
    "for i in range(len(testTexts)):\n",
    "    TEXT.append(testTexts[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on `texts token' (list of sample texts)`\n",
    "# return list of sequences (one per text)\n",
    "\n",
    "tokenizer.fit_on_texts(TEXT)\n",
    "sequences = tokenizer.texts_to_sequences(TEXT)\n",
    "sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "print(len(sequences))\n",
    "# sequence는 전체?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequences[:1088]\n",
    "y_train = to_categorical(trainLabels)\n",
    "\n",
    "x_val = sequences[1088:1280]\n",
    "y_val = to_categorical(validationLabels)\n",
    "\n",
    "x_test = sequences[1280:]\n",
    "y_test = to_categorical(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainTexts[0])\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map: word -> rank/index(int) in text\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_FEATURES, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_FEATURES:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where all models are saved\n",
    "BASE_PATH = './newsModel/'\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    os.mkdir(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint(model_name):\n",
    "    # creates a subdirectory under `BASE_PATH`\n",
    "    MODEL_PATH = os.path.join(BASE_PATH, model_name)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.mkdir(MODEL_PATH)\n",
    "    \n",
    "    return ModelCheckpoint(filepath=os.path.join(MODEL_PATH, '{epoch:02d}-{val_loss:.4f}.hdf5'),\n",
    "                           monitor='val_loss',\n",
    "                           verbose=1,\n",
    "                           save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_epochs = 30\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 1088 samples, validate on 192 samples\n",
      "Epoch 1/30\n",
      "1088/1088 [==============================] - 393s 361ms/step - loss: 2.0133 - acc: 0.1994 - val_loss: 1.8814 - val_acc: 0.2917\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.88136, saving model to ./newsModel/lstm\\01-1.8814.hdf5\n",
      "Epoch 2/30\n",
      "1088/1088 [==============================] - 476s 438ms/step - loss: 1.7962 - acc: 0.3153 - val_loss: 1.9442 - val_acc: 0.2917\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.88136\n",
      "Epoch 3/30\n",
      "1088/1088 [==============================] - 503s 462ms/step - loss: 1.7780 - acc: 0.3346 - val_loss: 1.7223 - val_acc: 0.3490\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.88136 to 1.72226, saving model to ./newsModel/lstm\\03-1.7223.hdf5\n",
      "Epoch 4/30\n",
      "1088/1088 [==============================] - 497s 457ms/step - loss: 1.7034 - acc: 0.3575 - val_loss: 1.6920 - val_acc: 0.3802\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.72226 to 1.69204, saving model to ./newsModel/lstm\\04-1.6920.hdf5\n",
      "Epoch 5/30\n",
      "1088/1088 [==============================] - 521s 479ms/step - loss: 1.6613 - acc: 0.3768 - val_loss: 1.6854 - val_acc: 0.3646\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.69204 to 1.68537, saving model to ./newsModel/lstm\\05-1.6854.hdf5\n",
      "Epoch 6/30\n",
      "1088/1088 [==============================] - 478s 440ms/step - loss: 1.6156 - acc: 0.3971 - val_loss: 1.6500 - val_acc: 0.3646\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.68537 to 1.64997, saving model to ./newsModel/lstm\\06-1.6500.hdf5\n",
      "Epoch 7/30\n",
      "1088/1088 [==============================] - 516s 474ms/step - loss: 1.5965 - acc: 0.3971 - val_loss: 1.6062 - val_acc: 0.3906\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.64997 to 1.60616, saving model to ./newsModel/lstm\\07-1.6062.hdf5\n",
      "Epoch 8/30\n",
      "1088/1088 [==============================] - 542s 498ms/step - loss: 1.5252 - acc: 0.4476 - val_loss: 1.4986 - val_acc: 0.4375\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.60616 to 1.49857, saving model to ./newsModel/lstm\\08-1.4986.hdf5\n",
      "Epoch 9/30\n",
      "1088/1088 [==============================] - 575s 528ms/step - loss: 1.4676 - acc: 0.4439 - val_loss: 1.5079 - val_acc: 0.4896\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.49857\n",
      "Epoch 10/30\n",
      "1088/1088 [==============================] - 587s 540ms/step - loss: 1.4269 - acc: 0.4761 - val_loss: 1.4735 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.49857 to 1.47349, saving model to ./newsModel/lstm\\10-1.4735.hdf5\n",
      "Epoch 11/30\n",
      "1088/1088 [==============================] - 623s 572ms/step - loss: 1.4255 - acc: 0.4844 - val_loss: 1.4365 - val_acc: 0.4896\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.47349 to 1.43645, saving model to ./newsModel/lstm\\11-1.4365.hdf5\n",
      "Epoch 12/30\n",
      "1088/1088 [==============================] - 633s 582ms/step - loss: 1.4111 - acc: 0.4715 - val_loss: 1.3732 - val_acc: 0.4896\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.43645 to 1.37324, saving model to ./newsModel/lstm\\12-1.3732.hdf5\n",
      "Epoch 13/30\n",
      "1088/1088 [==============================] - 641s 589ms/step - loss: 1.3421 - acc: 0.5028 - val_loss: 1.4065 - val_acc: 0.4427\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.37324\n",
      "Epoch 14/30\n",
      "1088/1088 [==============================] - 692s 636ms/step - loss: 1.3612 - acc: 0.4982 - val_loss: 1.4910 - val_acc: 0.4792\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.37324\n",
      "Epoch 15/30\n",
      "1088/1088 [==============================] - 590s 543ms/step - loss: 1.3501 - acc: 0.4825 - val_loss: 1.4586 - val_acc: 0.4740\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.37324\n",
      "Epoch 16/30\n",
      "1088/1088 [==============================] - 532s 489ms/step - loss: 1.3468 - acc: 0.4991 - val_loss: 1.3728 - val_acc: 0.5104\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.37324 to 1.37281, saving model to ./newsModel/lstm\\16-1.3728.hdf5\n",
      "Epoch 17/30\n",
      "1088/1088 [==============================] - 552s 508ms/step - loss: 1.2207 - acc: 0.5469 - val_loss: 1.4067 - val_acc: 0.4896\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.37281\n",
      "Epoch 18/30\n",
      "1088/1088 [==============================] - 548s 504ms/step - loss: 1.2322 - acc: 0.5506 - val_loss: 1.3410 - val_acc: 0.4896\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.37281 to 1.34096, saving model to ./newsModel/lstm\\18-1.3410.hdf5\n",
      "Epoch 19/30\n",
      "1088/1088 [==============================] - 460s 423ms/step - loss: 1.1733 - acc: 0.5735 - val_loss: 1.2901 - val_acc: 0.5677\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.34096 to 1.29013, saving model to ./newsModel/lstm\\19-1.2901.hdf5\n",
      "Epoch 20/30\n",
      "1088/1088 [==============================] - 395s 363ms/step - loss: 1.1305 - acc: 0.5717 - val_loss: 1.3247 - val_acc: 0.5365\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.29013\n",
      "Epoch 21/30\n",
      "1088/1088 [==============================] - 399s 366ms/step - loss: 1.2083 - acc: 0.5460 - val_loss: 1.3736 - val_acc: 0.5208\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.29013\n",
      "Epoch 22/30\n",
      "1088/1088 [==============================] - 416s 383ms/step - loss: 1.1653 - acc: 0.5754 - val_loss: 1.3288 - val_acc: 0.5052\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.29013\n",
      "Epoch 23/30\n",
      "1088/1088 [==============================] - 652s 600ms/step - loss: 1.2032 - acc: 0.5551 - val_loss: 1.4477 - val_acc: 0.4844\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.29013\n",
      "Epoch 24/30\n",
      "1088/1088 [==============================] - 875s 804ms/step - loss: 1.1479 - acc: 0.5671 - val_loss: 1.3152 - val_acc: 0.5469\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.29013\n",
      "Epoch 25/30\n",
      "1088/1088 [==============================] - 828s 761ms/step - loss: 1.0533 - acc: 0.6149 - val_loss: 1.3219 - val_acc: 0.5208\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.29013\n",
      "Epoch 26/30\n",
      "1088/1088 [==============================] - 786s 722ms/step - loss: 1.0098 - acc: 0.6287 - val_loss: 1.4006 - val_acc: 0.5208\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.29013\n",
      "Epoch 27/30\n",
      "1088/1088 [==============================] - 784s 721ms/step - loss: 1.0723 - acc: 0.6149 - val_loss: 1.3684 - val_acc: 0.5365\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.29013\n",
      "Epoch 28/30\n",
      "1088/1088 [==============================] - 780s 717ms/step - loss: 1.0152 - acc: 0.6222 - val_loss: 1.3258 - val_acc: 0.5573\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.29013\n",
      "Epoch 29/30\n",
      "1088/1088 [==============================] - 792s 728ms/step - loss: 0.9660 - acc: 0.6369 - val_loss: 1.4306 - val_acc: 0.4740\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.29013\n",
      "Epoch 30/30\n",
      "1088/1088 [==============================] - 655s 602ms/step - loss: 1.0339 - acc: 0.6241 - val_loss: 1.3769 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.29013\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "# train a 3-layer bi-LSTM model\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(embedded_sequences)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(128))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint = create_checkpoint('lstm')  # checkpoint callback\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=max_epochs,\n",
    "          validation_data=(x_val, y_val),\n",
    "          shuffle=True,\n",
    "          callbacks= [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training (optional)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(8,8))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 3.0])\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(embedded_sequences)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(128))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.load_weights('./newsModel/lstm/19-1.2901.hdf5')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test,\n",
    "                           batch_size=batch_size)\n",
    "\n",
    "print(f'----- Evaluation loss and metrics for {len(y_test)} test samples -----')\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
