{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir) \n",
    "import nltk, re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Dense, Input, Bidirectional, LSTM, Dropout, CuDNNLSTM\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Hannanum\n",
    "from gensim.models import FastText\n",
    "\n",
    "import pickle\n",
    "import codecs\n",
    "import csv\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = './newsData'\n",
    "directory_label = {0: '정치', 1: '경제', 2: '사회', 3 : '생활/문화', 4 : '세계', 5 : '기술/IT', 6 : '연예', 7 : '스포츠'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD_EMBEDDING_WORD2VEC = './wiki.ko/wiki.ko.bin'\n",
    "#WORD_EMBEDDING_WORD2VEC = './wiki.ko/wiki.ko.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "EMBEDDING_DIM = 300\n",
    "#EMBEDDING_DIM = 200\n",
    "#EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.15\n",
    "VALIDATION_SPLIT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konlp Objects\n",
    "komo = Komoran()\n",
    "twitter = Twitter()\n",
    "hannanum = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of texts\n",
    "Texts = []  \n",
    "trainTexts = []\n",
    "validationTexts = []\n",
    "testTexts = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of label ids\n",
    "Labels = []\n",
    "trainLabels = []  \n",
    "validationLabels = []\n",
    "testLabels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary mapping label name to numeric id\n",
    "labels_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence of texts\n",
    "seq = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load texts data\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[directory_label[int(name)]] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            fpath = os.path.join(path, fname)\n",
    "            args = {'encoding': 'utf-8'}\n",
    "            if(re.findall('.1[6789][0-9]NewsData',fpath)):\n",
    "                # test texts\n",
    "                text = []\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    #pos = twitter.pos(t.strip())\n",
    "                    #pos = hannanum.pos(t.strip())\n",
    "                    pos = komo.pos(t.strip())\n",
    "                    for pair in pos :\n",
    "                        #['NNG', 'NNP', 'VV', 'VA']: part-of-speech of content word\n",
    "                        if (re.findall('NNP|NNG',pair[1])):\n",
    "                            morpheme = pair[0]\n",
    "                            text.append(morpheme)\n",
    "                    testTexts.append(text)\n",
    "                testLabels.append(label_id)\n",
    "            else :\n",
    "                # train & validation texts\n",
    "                text = []\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    #pos = twitter.pos(t.strip())\n",
    "                    #pos = hannanum.pos(t.strip())\n",
    "                    pos = komo.pos(t.strip())\n",
    "                    for pair in pos :\n",
    "                        if (re.findall('NNP|NNG',pair[1])):\n",
    "                            morpheme = pair[0]\n",
    "                            text.append(morpheme)\n",
    "                    Texts.append(text)\n",
    "                Labels.append(label_id)\n",
    "            seq.append(text)\n",
    "print('Found %s texts.' % len(Texts))\n",
    "print('Found %s texts.' % len(testTexts))\n",
    "print(len(Labels))\n",
    "print(len(testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Texts[1])\n",
    "print(Labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_FEATURES = 10000\n",
    "MAX_FEATURES = 20000\n",
    "#MAX_FEATURES = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec\n",
    "model_ko = FastText(seq, min_count=5, size=EMBEDDING_DIM, window=5) \n",
    "words = list(model_ko.wv.vocab)\n",
    "embeddings_index = {}\n",
    "for i in words:\n",
    "    embeddings_index[i] = model_ko[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, validation_split in `fit` does not shuffle the data\n",
    "indices = np.arange(len(Texts), dtype=int)\n",
    "print(type(indices[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(Texts))\n",
    "print(indices)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "Texts = np.array(Texts)\n",
    "Labels = np.array(Labels)\n",
    "\n",
    "Texts = Texts[indices]\n",
    "Labels = Labels[indices]\n",
    "\n",
    "indices2 = np.arange(len(testTexts))\n",
    "np.random.shuffle(indices2)\n",
    "\n",
    "testTexts = np.array(testTexts)\n",
    "testLabels = np.array(testLabels)\n",
    "\n",
    "testTexts = testTexts[indices2]\n",
    "testLabels = testLabels[indices2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = int(VALIDATION_SPLIT * len(Texts))\n",
    "\n",
    "validationTexts = Texts[:num_validation_samples]\n",
    "validationLabels = Labels[:num_validation_samples]\n",
    "\n",
    "trainTexts = Texts[num_validation_samples:]\n",
    "trainLabels = Labels[num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainTexts))\n",
    "print(len(validationTexts))\n",
    "print(len(testTexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & validation texts의 길이 분포 list\n",
    "trainLen  = list()\n",
    "for i in range(len(trainTexts)):\n",
    "    trainLen.append(len(trainTexts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & validation texts의 길이 분포 list\n",
    "validationLen  = list()\n",
    "for i in range(len(validationTexts)):\n",
    "    validationLen.append(len(validationTexts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test texts의 길이 분포 list\n",
    "testLen  = list()\n",
    "for i in range(len(testTexts)):\n",
    "    testLen.append(len(testTexts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 text별로 길이 분포\n",
    "sns.distplot(trainLen, hist = True, rug =True);\n",
    "sns.distplot(validationLen, hist = True, rug =True);\n",
    "sns.distplot(testLen, hist= True, rug =True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_SEQUENCE_LENGTH = 150\n",
    "#MAX_SEQUENCE_LENGTH = 200\n",
    "#MAX_SEQUENCE_LENGTH = 250\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "#MAX_SEQUENCE_LENGTH = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 선언\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "TEXT = list()\n",
    "for i in range(len(trainTexts)):\n",
    "    TEXT.append(trainTexts[i])\n",
    "    \n",
    "for i in range(len(validationTexts)):\n",
    "    TEXT.append(validationTexts[i]) \n",
    "    \n",
    "for i in range(len(testTexts)):\n",
    "    TEXT.append(testTexts[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on `texts token' (list of sample texts)`\n",
    "# return list of sequences (one per text)\n",
    "\n",
    "tokenizer.fit_on_texts(TEXT)\n",
    "sequences = tokenizer.texts_to_sequences(TEXT)\n",
    "sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "print(len(sequences))\n",
    "# sequence는 전체?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequences[:1088]\n",
    "y_train = to_categorical(trainLabels)\n",
    "\n",
    "x_val = sequences[1088:1280]\n",
    "y_val = to_categorical(validationLabels)\n",
    "\n",
    "x_test = sequences[1280:]\n",
    "y_test = to_categorical(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(trainTexts[0])\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map: word -> rank/index(int) in text\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_FEATURES, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_FEATURES:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where all models are saved\n",
    "BASE_PATH = './newsModel/'\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    os.mkdir(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint(model_name):\n",
    "    # creates a subdirectory under `BASE_PATH`\n",
    "    MODEL_PATH = os.path.join(BASE_PATH, model_name)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.mkdir(MODEL_PATH)\n",
    "    \n",
    "    return ModelCheckpoint(filepath=os.path.join(MODEL_PATH, '{epoch:02d}-{val_loss:.4f}.hdf5'),\n",
    "                           monitor='val_loss',\n",
    "                           verbose=1,\n",
    "                           save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_epochs = 200\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training...')\n",
    "\n",
    "# train a 3-layer bi-LSTM model\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(embedded_sequences)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(CuDNNLSTM(32))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint = create_checkpoint('lstm')  # checkpoint callback\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=max_epochs,\n",
    "          validation_data=(x_val, y_val),\n",
    "          shuffle=True,\n",
    "          callbacks= [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training (optional)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(8,8))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 3.0])\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test,\n",
    "                           batch_size=batch_size)\n",
    "\n",
    "print(f'----- Evaluation loss and metrics for {len(y_test)} test samples -----')\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
