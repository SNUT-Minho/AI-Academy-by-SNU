{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\MinhoLee\\Anaconda3\\envs\\venv\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir) \n",
    "import nltk, re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Dense, Input, Bidirectional, LSTM, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Hannanum\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pickle\n",
    "import codecs\n",
    "import csv\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = './newsData'\n",
    "directory_label = {0: '정치', 1: '경제', 2: '사회', 3 : '생활/문화', 4 : '세계', 5 : '기술/IT', 6 : '연예', 7 : '스포츠'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD_EMBEDDING_WORD2VEC = './wiki.ko/wiki.ko.bin'\n",
    "#WORD_EMBEDDING_WORD2VEC = './wiki.ko/wiki.ko.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "EMBEDDING_DIM = 100\n",
    "#EMBEDDING_DIM = 200\n",
    "#EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.15\n",
    "VALIDATION_SPLIT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konlp Objects\n",
    "komo = Komoran()\n",
    "twitter = Twitter()\n",
    "hannanum = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of texts\n",
    "Texts = []  \n",
    "trainTexts = []\n",
    "validationTexts = []\n",
    "testTexts = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of label ids\n",
    "Labels = []\n",
    "trainLabels = []  \n",
    "validationLabels = []\n",
    "testLabels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary mapping label name to numeric id\n",
    "labels_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence of texts\n",
    "seq = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(Texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load texts data\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[directory_label[int(name)]] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            fpath = os.path.join(path, fname)\n",
    "            args = {'encoding': 'utf-8'}\n",
    "            if(re.findall('.1[6789][0-9]NewsData',fpath)):\n",
    "                # test texts\n",
    "                text = []\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    #pos = twitter.pos(t.strip())\n",
    "                    #pos = hannanum.pos(t.strip())\n",
    "                    pos = komo.pos(t.strip())\n",
    "                    for pair in pos :\n",
    "                        #['NNG', 'NNP', 'VV', 'VA']: part-of-speech of content word\n",
    "                        if (re.findall('^[NV]',pair[1])):\n",
    "                            morpheme = pair[0]+'/'+pair[1]\n",
    "                            text.append(morpheme)\n",
    "                    testTexts.append(text)\n",
    "                testLabels.append(label_id)\n",
    "            else :\n",
    "                # train & validation texts\n",
    "                text = []\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    #pos = twitter.pos(t.strip())\n",
    "                    #pos = hannanum.pos(t.strip())\n",
    "                    pos = komo.pos(t.strip())\n",
    "                    for pair in pos :\n",
    "                        if (re.findall('^[NV]',pair[1])):\n",
    "                            morpheme = pair[0]+'/'+pair[1]\n",
    "                            text.append(morpheme)\n",
    "                    Texts.append(text)\n",
    "                Labels.append(label_id)\n",
    "            seq.append(text)\n",
    "print('Found %s texts.' % len(Texts))\n",
    "print('Found %s texts.' % len(testTexts))\n",
    "print(len(Labels))\n",
    "print(len(testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "'''\n",
    "print(\"mean : \", np.mean(trainLen))\n",
    "print(\"max : \" ,np.max(trainLen))\n",
    "print(\"min : \" ,np.min(trainLen))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_FEATURES = 10000\n",
    "MAX_FEATURES = 20000\n",
    "#MAX_FEATURES = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec\n",
    "model_ko = Word2Vec(seq, min_count=1, size=EMBEDDING_DIM) \n",
    "words = list(model_ko.wv.vocab)\n",
    "embeddings_index = {}\n",
    "for i in words:\n",
    "    embeddings_index[i] = model_ko[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, validation_split in `fit` does not shuffle the data\n",
    "indices = np.arange(len(Texts), dtype=int)\n",
    "print(type(indices[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(Texts))\n",
    "print(indices)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "Texts = np.array(Texts)\n",
    "Labels = np.array(Labels)\n",
    "\n",
    "Texts = Texts[indices]\n",
    "Labels = Labels[indices]\n",
    "\n",
    "indices2 = np.arange(len(testTexts))\n",
    "np.random.shuffle(indices2)\n",
    "\n",
    "testTexts = np.array(testTexts)\n",
    "testLabels = np.array(testLabels)\n",
    "\n",
    "testTexts = testTexts[indices2]\n",
    "testLabels = testLabels[indices2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = int(VALIDATION_SPLIT * len(Texts))\n",
    "\n",
    "validationTexts = Texts[:num_validation_samples]\n",
    "validationLabels = Labels[:num_validation_samples]\n",
    "\n",
    "trainTexts = Texts[num_validation_samples:]\n",
    "trainLabels = Labels[num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainTexts))\n",
    "print(len(validationTexts))\n",
    "print(len(testTexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & validation texts의 길이 분포 list\n",
    "trainLen  = list()\n",
    "for i in range(len(trainTexts)):\n",
    "    trainLen.append(len(trainTexts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & validation texts의 길이 분포 list\n",
    "validationLen  = list()\n",
    "for i in range(len(validationTexts)):\n",
    "    validationLen.append(len(validationTexts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test texts의 길이 분포 list\n",
    "testLen  = list()\n",
    "for i in range(len(testTexts)):\n",
    "    testLen.append(len(testTexts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 text별로 길이 분포\n",
    "sns.distplot(trainLen, hist = True, rug =True);\n",
    "sns.distplot(validationLen, hist = True, rug =True);\n",
    "sns.distplot(testLen, hist= True, rug =True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 150\n",
    "#MAX_SEQUENCE_LENGTH = 200\n",
    "#MAX_SEQUENCE_LENGTH = 250\n",
    "#MAX_SEQUENCE_LENGTH = 300\n",
    "#MAX_SEQUENCE_LENGTH = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 선언\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "TEXT = list()\n",
    "for i in range(len(trainTexts)):\n",
    "    TEXT.append(trainTexts[i])\n",
    "    \n",
    "for i in range(len(validationTexts)):\n",
    "    TEXT.append(validationTexts[i]) \n",
    "    \n",
    "for i in range(len(testTexts)):\n",
    "    TEXT.append(testTexts[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on `texts token' (list of sample texts)`\n",
    "# return list of sequences (one per text)\n",
    "\n",
    "tokenizer.fit_on_texts(TEXT)\n",
    "sequences = tokenizer.texts_to_sequences(TEXT)\n",
    "sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "print(len(sequences))\n",
    "# sequence는 전체?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequences[:1088]\n",
    "y_train = to_categorical(trainLabels)\n",
    "\n",
    "x_val = sequences[1088:1280]\n",
    "y_val = to_categorical(validationLabels)\n",
    "\n",
    "x_test = sequences[1280:]\n",
    "y_test = to_categorical(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainTexts[0])\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map: word -> rank/index(int) in text\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_FEATURES, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_FEATURES:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where all models are saved\n",
    "BASE_PATH = './newsModel/'\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    os.mkdir(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint(model_name):\n",
    "    # creates a subdirectory under `BASE_PATH`\n",
    "    MODEL_PATH = os.path.join(BASE_PATH, model_name)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.mkdir(MODEL_PATH)\n",
    "    \n",
    "    return ModelCheckpoint(filepath=os.path.join(MODEL_PATH, '{epoch:02d}-{val_loss:.4f}.hdf5'),\n",
    "                           monitor='val_loss',\n",
    "                           verbose=1,\n",
    "                           save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_epochs = 30\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 1088 samples, validate on 192 samples\n",
      "Epoch 1/30\n",
      "1088/1088 [==============================] - 207s 190ms/step - loss: 2.0387 - acc: 0.1700 - val_loss: 1.8442 - val_acc: 0.2604\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.84425, saving model to ./newsModel/lstm\\01-1.8442.hdf5\n",
      "Epoch 2/30\n",
      "1088/1088 [==============================] - 173s 159ms/step - loss: 1.8431 - acc: 0.2877 - val_loss: 1.6955 - val_acc: 0.3281\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.84425 to 1.69551, saving model to ./newsModel/lstm\\02-1.6955.hdf5\n",
      "Epoch 3/30\n",
      "1088/1088 [==============================] - 175s 161ms/step - loss: 1.7281 - acc: 0.3539 - val_loss: 1.6223 - val_acc: 0.3958\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.69551 to 1.62228, saving model to ./newsModel/lstm\\03-1.6223.hdf5\n",
      "Epoch 4/30\n",
      "1088/1088 [==============================] - 173s 159ms/step - loss: 1.6352 - acc: 0.3888 - val_loss: 1.5612 - val_acc: 0.3958\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.62228 to 1.56117, saving model to ./newsModel/lstm\\04-1.5612.hdf5\n",
      "Epoch 5/30\n",
      "1088/1088 [==============================] - 177s 163ms/step - loss: 1.5925 - acc: 0.4035 - val_loss: 1.4835 - val_acc: 0.4531\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.56117 to 1.48351, saving model to ./newsModel/lstm\\05-1.4835.hdf5\n",
      "Epoch 6/30\n",
      "1088/1088 [==============================] - 180s 166ms/step - loss: 1.5578 - acc: 0.4283 - val_loss: 1.5386 - val_acc: 0.4271\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.48351\n",
      "Epoch 7/30\n",
      "1088/1088 [==============================] - 177s 162ms/step - loss: 1.5079 - acc: 0.4449 - val_loss: 1.4835 - val_acc: 0.4427\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.48351\n",
      "Epoch 8/30\n",
      "1088/1088 [==============================] - 188s 173ms/step - loss: 1.4187 - acc: 0.4614 - val_loss: 1.4851 - val_acc: 0.4688\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.48351\n",
      "Epoch 9/30\n",
      "1088/1088 [==============================] - 184s 169ms/step - loss: 1.4114 - acc: 0.4871 - val_loss: 1.5687 - val_acc: 0.4167\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.48351\n",
      "Epoch 10/30\n",
      "1088/1088 [==============================] - 179s 164ms/step - loss: 1.4009 - acc: 0.4899 - val_loss: 1.4235 - val_acc: 0.4531\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.48351 to 1.42347, saving model to ./newsModel/lstm\\10-1.4235.hdf5\n",
      "Epoch 11/30\n",
      "1088/1088 [==============================] - 178s 164ms/step - loss: 1.3938 - acc: 0.4881 - val_loss: 1.4357 - val_acc: 0.4375\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.42347\n",
      "Epoch 12/30\n",
      "1088/1088 [==============================] - 181s 167ms/step - loss: 1.3607 - acc: 0.4945 - val_loss: 1.4319 - val_acc: 0.4635\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.42347\n",
      "Epoch 13/30\n",
      "1088/1088 [==============================] - 182s 168ms/step - loss: 1.2865 - acc: 0.5303 - val_loss: 1.4588 - val_acc: 0.4688\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.42347\n",
      "Epoch 14/30\n",
      "1088/1088 [==============================] - 179s 164ms/step - loss: 1.2797 - acc: 0.5276 - val_loss: 1.4548 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.42347\n",
      "Epoch 15/30\n",
      "1088/1088 [==============================] - 179s 164ms/step - loss: 1.2341 - acc: 0.5515 - val_loss: 1.4150 - val_acc: 0.4688\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.42347 to 1.41499, saving model to ./newsModel/lstm\\15-1.4150.hdf5\n",
      "Epoch 16/30\n",
      "1088/1088 [==============================] - 180s 166ms/step - loss: 1.2758 - acc: 0.5368 - val_loss: 1.5735 - val_acc: 0.4115\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.41499\n",
      "Epoch 17/30\n",
      "1088/1088 [==============================] - 185s 170ms/step - loss: 1.3061 - acc: 0.5147 - val_loss: 1.4853 - val_acc: 0.4375\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.41499\n",
      "Epoch 18/30\n",
      "1088/1088 [==============================] - 187s 172ms/step - loss: 1.2410 - acc: 0.5441 - val_loss: 1.4189 - val_acc: 0.4844\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.41499\n",
      "Epoch 19/30\n",
      "1088/1088 [==============================] - 183s 168ms/step - loss: 1.2167 - acc: 0.5515 - val_loss: 1.4233 - val_acc: 0.5052\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.41499\n",
      "Epoch 20/30\n",
      "1088/1088 [==============================] - 185s 170ms/step - loss: 1.1973 - acc: 0.5524 - val_loss: 1.3490 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.41499 to 1.34903, saving model to ./newsModel/lstm\\20-1.3490.hdf5\n",
      "Epoch 21/30\n",
      "1088/1088 [==============================] - 185s 170ms/step - loss: 1.1087 - acc: 0.5956 - val_loss: 1.4230 - val_acc: 0.4896\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.34903\n",
      "Epoch 22/30\n",
      "1088/1088 [==============================] - 181s 166ms/step - loss: 1.0678 - acc: 0.6103 - val_loss: 1.5095 - val_acc: 0.4479\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.34903\n",
      "Epoch 23/30\n",
      "1088/1088 [==============================] - 185s 170ms/step - loss: 1.0653 - acc: 0.5947 - val_loss: 1.4575 - val_acc: 0.5052\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.34903\n",
      "Epoch 24/30\n",
      "1088/1088 [==============================] - 180s 165ms/step - loss: 1.0800 - acc: 0.6204 - val_loss: 1.4538 - val_acc: 0.4740\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.34903\n",
      "Epoch 25/30\n",
      "1088/1088 [==============================] - 181s 166ms/step - loss: 1.0243 - acc: 0.6176 - val_loss: 1.4647 - val_acc: 0.4740\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.34903\n",
      "Epoch 26/30\n",
      "1088/1088 [==============================] - 182s 167ms/step - loss: 1.0305 - acc: 0.6029 - val_loss: 1.3823 - val_acc: 0.5052\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.34903\n",
      "Epoch 27/30\n",
      "1088/1088 [==============================] - 201s 185ms/step - loss: 0.9858 - acc: 0.6369 - val_loss: 1.3963 - val_acc: 0.5260\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.34903\n",
      "Epoch 28/30\n",
      "1088/1088 [==============================] - 199s 183ms/step - loss: 0.9034 - acc: 0.6654 - val_loss: 1.3922 - val_acc: 0.5312\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.34903\n",
      "Epoch 29/30\n",
      "1088/1088 [==============================] - 189s 174ms/step - loss: 0.8748 - acc: 0.6866 - val_loss: 1.4882 - val_acc: 0.4792\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.34903\n",
      "Epoch 30/30\n",
      "1088/1088 [==============================] - 193s 177ms/step - loss: 0.8235 - acc: 0.6967 - val_loss: 1.4796 - val_acc: 0.4740\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.34903\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "# train a 3-layer bi-LSTM model\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(embedded_sequences)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(128))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint = create_checkpoint('lstm')  # checkpoint callback\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=max_epochs,\n",
    "          validation_data=(x_val, y_val),\n",
    "          shuffle=True,\n",
    "          callbacks= [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training (optional)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(8,8))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 3.0])\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 4s 13ms/step\n",
      "----- Evaluation loss and metrics for 320 test samples -----\n",
      "Test loss: 1.8770819187164307\n",
      "Test accuracy: 0.3\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test,\n",
    "                           batch_size=batch_size)\n",
    "\n",
    "print(f'----- Evaluation loss and metrics for {len(y_test)} test samples -----')\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
