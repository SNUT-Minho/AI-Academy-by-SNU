{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir) \n",
    "import nltk, re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Dense, Input, Bidirectional, LSTM, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Hannanum\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pickle\n",
    "import codecs\n",
    "import csv\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = './newsData'\n",
    "directory_label = {0: '정치', 1: '경제', 2: '사회', 3 : '생활/문화', 4 : '세계', 5 : '기술/IT', 6 : '연예', 7 : '스포츠'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD_EMBEDDING_WORD2VEC = './wiki.ko/wiki.ko.bin'\n",
    "#WORD_EMBEDDING_WORD2VEC = './wiki.ko/wiki.ko.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "EMBEDDING_DIM = 100\n",
    "#EMBEDDING_DIM = 200\n",
    "#EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.15\n",
    "VALIDATION_SPLIT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konlp Objects\n",
    "komo = Komoran()\n",
    "twitter = Twitter()\n",
    "hannanum = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of texts\n",
    "Texts = []  \n",
    "trainTexts = []\n",
    "validationTexts = []\n",
    "testTexts = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of label ids\n",
    "Labels = []\n",
    "trainLabels = []  \n",
    "validationLabels = []\n",
    "testLabels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary mapping label name to numeric id\n",
    "labels_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence of texts\n",
    "seq = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load texts data\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[directory_label[int(name)]] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            fpath = os.path.join(path, fname)\n",
    "            args = {'encoding': 'utf-8'}\n",
    "            if(re.findall('.1[6789][0-9]NewsData',fpath)):\n",
    "                # test texts\n",
    "                text = []\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    #pos = twitter.pos(t.strip())\n",
    "                    #pos = hannanum.pos(t.strip())\n",
    "                    pos = komo.pos(t.strip())\n",
    "                    for pair in pos :\n",
    "                        #['NNG', 'NNP', 'VV', 'VA']: part-of-speech of content word\n",
    "                        if (re.findall('^[NV]',pair[1])):\n",
    "                            morpheme = pair[0]+'/'+pair[1]\n",
    "                            text.append(morpheme)\n",
    "                    testTexts.append(text)\n",
    "                testLabels.append(label_id)\n",
    "            else :\n",
    "                # train & validation texts\n",
    "                text = []\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    #pos = twitter.pos(t.strip())\n",
    "                    #pos = hannanum.pos(t.strip())\n",
    "                    pos = komo.pos(t.strip())\n",
    "                    for pair in pos :\n",
    "                        if (re.findall('^[NV]',pair[1])):\n",
    "                            morpheme = pair[0]+'/'+pair[1]\n",
    "                            text.append(morpheme)\n",
    "                    Texts.append(text)\n",
    "                Labels.append(label_id)\n",
    "            seq.append(text)\n",
    "print('Found %s texts.' % len(Texts))\n",
    "print('Found %s texts.' % len(testTexts))\n",
    "print(len(Labels))\n",
    "print(len(testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "'''\n",
    "print(\"mean : \", np.mean(trainLen))\n",
    "print(\"max : \" ,np.max(trainLen))\n",
    "print(\"min : \" ,np.min(trainLen))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_FEATURES = 10000\n",
    "MAX_FEATURES = 20000\n",
    "#MAX_FEATURES = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec\n",
    "model_ko = Word2Vec(seq, min_count=1, size=EMBEDDING_DIM) \n",
    "words = list(model_ko.wv.vocab)\n",
    "embeddings_index = {}\n",
    "for i in words:\n",
    "    embeddings_index[i] = model_ko[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, validation_split in `fit` does not shuffle the data\n",
    "indices = np.arange(len(Texts), dtype=int)\n",
    "print(type(indices[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(Texts))\n",
    "print(indices)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "Texts = np.array(Texts)\n",
    "Labels = np.array(Labels)\n",
    "\n",
    "Texts = Texts[indices]\n",
    "Labels = Labels[indices]\n",
    "\n",
    "indices2 = np.arange(len(testTexts))\n",
    "np.random.shuffle(indices2)\n",
    "\n",
    "testTexts = np.array(testTexts)\n",
    "testLabels = np.array(testLabels)\n",
    "\n",
    "testTexts = testTexts[indices2]\n",
    "testLabels = testLabels[indices2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = int(VALIDATION_SPLIT * len(Texts))\n",
    "\n",
    "validationTexts = Texts[:num_validation_samples]\n",
    "validationLabels = Labels[:num_validation_samples]\n",
    "\n",
    "trainTexts = Texts[num_validation_samples:]\n",
    "trainLabels = Labels[num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainTexts))\n",
    "print(len(validationTexts))\n",
    "print(len(testTexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & validation texts의 길이 분포 list\n",
    "trainLen  = list()\n",
    "for i in range(len(trainTexts)):\n",
    "    trainLen.append(len(trainTexts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & validation texts의 길이 분포 list\n",
    "validationLen  = list()\n",
    "for i in range(len(validationTexts)):\n",
    "    validationLen.append(len(validationTexts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test texts의 길이 분포 list\n",
    "testLen  = list()\n",
    "for i in range(len(testTexts)):\n",
    "    testLen.append(len(testTexts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 text별로 길이 분포\n",
    "sns.distplot(trainLen, hist = True, rug =True);\n",
    "sns.distplot(validationLen, hist = True, rug =True);\n",
    "sns.distplot(testLen, hist= True, rug =True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_SEQUENCE_LENGTH = 150\n",
    "#MAX_SEQUENCE_LENGTH = 200\n",
    "#MAX_SEQUENCE_LENGTH = 250\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "#MAX_SEQUENCE_LENGTH = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 선언\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "TEXT = list()\n",
    "for i in range(len(trainTexts)):\n",
    "    TEXT.append(trainTexts[i])\n",
    "    \n",
    "for i in range(len(validationTexts)):\n",
    "    TEXT.append(validationTexts[i]) \n",
    "    \n",
    "for i in range(len(testTexts)):\n",
    "    TEXT.append(testTexts[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on `texts token' (list of sample texts)`\n",
    "# return list of sequences (one per text)\n",
    "\n",
    "tokenizer.fit_on_texts(TEXT)\n",
    "sequences = tokenizer.texts_to_sequences(TEXT)\n",
    "sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "print(len(sequences))\n",
    "# sequence는 전체?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequences[:1088]\n",
    "y_train = to_categorical(trainLabels)\n",
    "\n",
    "x_val = sequences[1088:1280]\n",
    "y_val = to_categorical(validationLabels)\n",
    "\n",
    "x_test = sequences[1280:]\n",
    "y_test = to_categorical(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainTexts[0])\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map: word -> rank/index(int) in text\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_FEATURES, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_FEATURES:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where all models are saved\n",
    "BASE_PATH = './newsModel/'\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    os.mkdir(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoint(model_name):\n",
    "    # creates a subdirectory under `BASE_PATH`\n",
    "    MODEL_PATH = os.path.join(BASE_PATH, model_name)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.mkdir(MODEL_PATH)\n",
    "    \n",
    "    return ModelCheckpoint(filepath=os.path.join(MODEL_PATH, '{epoch:02d}-{val_loss:.4f}.hdf5'),\n",
    "                           monitor='val_loss',\n",
    "                           verbose=1,\n",
    "                           save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_epochs = 30\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 1088 samples, validate on 192 samples\n",
      "Epoch 1/30\n",
      "1088/1088 [==============================] - 139s 128ms/step - loss: 2.2903 - acc: 0.1682 - val_loss: 2.1041 - val_acc: 0.2083\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.10412, saving model to ./newsModel/lstm\\01-2.1041.hdf5\n",
      "Epoch 2/30\n",
      "1088/1088 [==============================] - 112s 103ms/step - loss: 1.9911 - acc: 0.2178 - val_loss: 2.2598 - val_acc: 0.1875\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.10412\n",
      "Epoch 3/30\n",
      "1088/1088 [==============================] - 112s 103ms/step - loss: 1.9273 - acc: 0.2776 - val_loss: 1.9431 - val_acc: 0.2188\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.10412 to 1.94311, saving model to ./newsModel/lstm\\03-1.9431.hdf5\n",
      "Epoch 4/30\n",
      "1088/1088 [==============================] - 121s 111ms/step - loss: 1.7888 - acc: 0.3373 - val_loss: 1.9978 - val_acc: 0.2656\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.94311\n",
      "Epoch 5/30\n",
      "1088/1088 [==============================] - 151s 138ms/step - loss: 1.7576 - acc: 0.3281 - val_loss: 1.8763 - val_acc: 0.3073\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.94311 to 1.87633, saving model to ./newsModel/lstm\\05-1.8763.hdf5\n",
      "Epoch 6/30\n",
      "1088/1088 [==============================] - 176s 162ms/step - loss: 1.7223 - acc: 0.3649 - val_loss: 1.8494 - val_acc: 0.2292\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.87633 to 1.84941, saving model to ./newsModel/lstm\\06-1.8494.hdf5\n",
      "Epoch 7/30\n",
      "1088/1088 [==============================] - 180s 165ms/step - loss: 1.5983 - acc: 0.4044 - val_loss: 1.9088 - val_acc: 0.2552\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.84941\n",
      "Epoch 8/30\n",
      "1088/1088 [==============================] - 182s 167ms/step - loss: 1.6668 - acc: 0.3805 - val_loss: 1.7016 - val_acc: 0.3281\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.84941 to 1.70162, saving model to ./newsModel/lstm\\08-1.7016.hdf5\n",
      "Epoch 9/30\n",
      "1088/1088 [==============================] - 181s 167ms/step - loss: 1.5179 - acc: 0.4182 - val_loss: 1.8591 - val_acc: 0.3177\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.70162\n",
      "Epoch 10/30\n",
      "1088/1088 [==============================] - 182s 167ms/step - loss: 1.5021 - acc: 0.4522 - val_loss: 1.6412 - val_acc: 0.3802\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.70162 to 1.64121, saving model to ./newsModel/lstm\\10-1.6412.hdf5\n",
      "Epoch 11/30\n",
      "1088/1088 [==============================] - 174s 160ms/step - loss: 1.4858 - acc: 0.4586 - val_loss: 1.6858 - val_acc: 0.3750\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.64121\n",
      "Epoch 12/30\n",
      "1088/1088 [==============================] - 184s 169ms/step - loss: 1.3953 - acc: 0.4954 - val_loss: 1.5632 - val_acc: 0.4010\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.64121 to 1.56325, saving model to ./newsModel/lstm\\12-1.5632.hdf5\n",
      "Epoch 13/30\n",
      "1088/1088 [==============================] - 187s 171ms/step - loss: 1.3329 - acc: 0.5129 - val_loss: 1.6604 - val_acc: 0.3594\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.56325\n",
      "Epoch 14/30\n",
      "1088/1088 [==============================] - 185s 170ms/step - loss: 1.3470 - acc: 0.5119 - val_loss: 1.6450 - val_acc: 0.3542\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.56325\n",
      "Epoch 15/30\n",
      "1088/1088 [==============================] - 183s 169ms/step - loss: 1.2837 - acc: 0.5414 - val_loss: 1.5587 - val_acc: 0.4010\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.56325 to 1.55866, saving model to ./newsModel/lstm\\15-1.5587.hdf5\n",
      "Epoch 16/30\n",
      "1088/1088 [==============================] - 184s 169ms/step - loss: 1.2076 - acc: 0.5634 - val_loss: 1.7519 - val_acc: 0.3958\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.55866\n",
      "Epoch 17/30\n",
      "1088/1088 [==============================] - 192s 176ms/step - loss: 1.2202 - acc: 0.5597 - val_loss: 1.5696 - val_acc: 0.4115\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.55866\n",
      "Epoch 18/30\n",
      "1088/1088 [==============================] - 185s 170ms/step - loss: 1.2198 - acc: 0.5653 - val_loss: 1.6276 - val_acc: 0.3854\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.55866\n",
      "Epoch 19/30\n",
      "1088/1088 [==============================] - 186s 171ms/step - loss: 1.1386 - acc: 0.5892 - val_loss: 1.6987 - val_acc: 0.3750\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.55866\n",
      "Epoch 20/30\n",
      "1088/1088 [==============================] - 189s 174ms/step - loss: 1.2012 - acc: 0.5680 - val_loss: 1.6122 - val_acc: 0.4010\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.55866\n",
      "Epoch 21/30\n",
      "1088/1088 [==============================] - 193s 178ms/step - loss: 1.0308 - acc: 0.6278 - val_loss: 1.7936 - val_acc: 0.3750\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.55866\n",
      "Epoch 22/30\n",
      "1088/1088 [==============================] - 189s 174ms/step - loss: 1.2392 - acc: 0.5414 - val_loss: 1.6147 - val_acc: 0.3958\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.55866\n",
      "Epoch 23/30\n",
      "1088/1088 [==============================] - 192s 176ms/step - loss: 1.0154 - acc: 0.6278 - val_loss: 1.5430 - val_acc: 0.4323\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.55866 to 1.54297, saving model to ./newsModel/lstm\\23-1.5430.hdf5\n",
      "Epoch 24/30\n",
      "1088/1088 [==============================] - 190s 175ms/step - loss: 0.9442 - acc: 0.6507 - val_loss: 1.7414 - val_acc: 0.4375\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.54297\n",
      "Epoch 25/30\n",
      "1088/1088 [==============================] - 192s 176ms/step - loss: 0.9889 - acc: 0.6452 - val_loss: 1.7849 - val_acc: 0.3802\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.54297\n",
      "Epoch 26/30\n",
      "1088/1088 [==============================] - 190s 175ms/step - loss: 0.9298 - acc: 0.6673 - val_loss: 1.6955 - val_acc: 0.4271\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.54297\n",
      "Epoch 27/30\n",
      "1088/1088 [==============================] - 193s 178ms/step - loss: 0.8691 - acc: 0.7013 - val_loss: 1.5767 - val_acc: 0.4479\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.54297\n",
      "Epoch 28/30\n",
      "1088/1088 [==============================] - 187s 172ms/step - loss: 0.8004 - acc: 0.7252 - val_loss: 1.7010 - val_acc: 0.4167\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.54297\n",
      "Epoch 29/30\n",
      "1088/1088 [==============================] - 196s 180ms/step - loss: 0.8012 - acc: 0.7114 - val_loss: 1.7804 - val_acc: 0.4271\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.54297\n",
      "Epoch 30/30\n",
      "1088/1088 [==============================] - 199s 183ms/step - loss: 0.8125 - acc: 0.7050 - val_loss: 1.7686 - val_acc: 0.4115\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.54297\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "# train a 3-layer bi-LSTM model\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(embedded_sequences)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(128))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint = create_checkpoint('lstm')  # checkpoint callback\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=max_epochs,\n",
    "          validation_data=(x_val, y_val),\n",
    "          shuffle=True,\n",
    "          callbacks= [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training (optional)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(8,8))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 3.0])\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(embedded_sequences)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(128))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.load_weights('./newsModel/lstm/38-1.2676.hdf5')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 11s 34ms/step\n",
      "----- Evaluation loss and metrics for 320 test samples -----\n",
      "Test loss: 1.683783721923828\n",
      "Test accuracy: 0.38125\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test,\n",
    "                           batch_size=batch_size)\n",
    "\n",
    "print(f'----- Evaluation loss and metrics for {len(y_test)} test samples -----')\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
