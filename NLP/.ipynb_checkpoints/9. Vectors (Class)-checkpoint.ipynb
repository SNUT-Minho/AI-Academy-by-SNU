{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07-2 vector semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 07.2 에서 39p부터 시작\n",
    "- cos으로 similiary를 평가하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Distirbution을 measure하는 방법 (41p)\n",
    "- Term frequency: 문서에서 실제로 나타난 것으로 frequency를 구함\n",
    "- Inverse document frequency: 특정 단어 i를 가지고 있는 document (전체 document에서 특정 document에만 나오는 단어라면 해당 document가 특별)\n",
    "- 45p에서 skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation of word representations in vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 벡터로 어떻게 단어를 estimation 할 것이냐?\n",
    "- n-gram 모델에서는 dimensionality가 작다는 문제가 있다.\n",
    "- word to vec같은거랑 비교하면 vec은 100차원 뭐이렇게 가는데 n-gram은 그렇지 않다\n",
    "- 5p 단어 하나를 분산된 distribution feature를 가진 vector로 표현하자\n",
    "- 유사한 단어끼리 vector cosine simliarity를 할수 있게 된다 (n-gram 같은거는 불가능했다)\n",
    "- n-gram을 예로들면 그냥 사전에 500번째 index가 genesis를 의미 \n",
    "- 근데 이걸 vector distribution으로 표현하면 -> 의미가 생김 \n",
    "- one-hot encoding은 벡터간 내적을 할 수가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- word vector는 다양한 degree에서 similiary를 비교할 수 있다.\n",
    "- 예시로 king vector에서 man을 뺴고 woman을 더하면 queen vector와 거의 유사하다\n",
    "- 그럼 어떻게 이런 weight를 얻을 것이냐? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feedfoward Nueral Network language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 단어에 대한 distribution을 nueral net으로 만들어보자\n",
    "- 기존 단어의 vector를 nueral net에 넣어서 다음단어를 예측하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 기존 history를 반영해서 시계열 데이터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. New Log-Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 수학적으로 Feedfoward와 rnn이 가지고 있는 비효율적인 계산을 개선해보자!\n",
    "- a) Continuous Bag of word: hidden layer가 없다. 그럼 학습없이 어떻게 weight를 결정하는가?\n",
    "- 사실 이건 딥러닝이라고 볼 수 없다?\n",
    "- !! 어떤 특정 단어를 estimation하는데 주변단어 확률로부터 중심단어 확률을 결정한다.\n",
    "\n",
    "- b) Continuous skip gram은 위와 똑같은데 방향이반대임 \n",
    "- 중심단어를 가지고 주변 단어들의 확률을 찾음\n",
    "- 일반적인 사고로는 a번이 더 논리적인것 같지만 실제로는 b가 성능이 더좋음\n",
    "- a,b 둘다 word feature distributino을 찾는 과정 - > vector로 변환\n",
    "- 20p에서 25p로 건너뜀\n",
    "- 그러고 그냥 끝남; 사진좀 보다가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 각 단어들별 벡터의 내적의 코사인 유사도가 최대가 되도록..\n",
    "- 5p에서 처음 input되는 단어가 love \n",
    "- 마지막 Wt-3 / Wt-2 / Wt-1 이 왼쪽에 나타나는 단어 ?\n",
    "- 실제 구해진 softmax 확률이랑 왼쪽에 있는 단어들이랑 비교해서 맞으면 잘 예측한거고\n",
    "- 실패했으면 loss function을 가지고 왼쪽으로 back propagation해서 nueral network처럼 다시 푼다\n",
    "- 8로 건너뜀\n",
    "- 이걸 할때는 window를 정해야된다 (양옆으로 몇개의 단어를 볼 것인가?)\n",
    "\n",
    "- skip-gram은 c-bow보다 학습량이 4배가 더크다 해당 예시에서(quick을 보면 4번이나 학습된다)\n",
    "- 분모를 줄이기 위해서는 등장하지않는 단어들의 내적값을 줄이는 방향으로 학습한다?\n",
    "- 분자는 내적이 커지도록 -> 해당 확률값이 커지도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 12p Vocabulary가 10만 / dimension이 10일때\n",
    "- 자주 등장하는거는 이미 학습이 많이 되었으니깐 조금 빼버리자 (subsampling freqeunt words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- negative sampling : softmax를 n개 단어를 다하지 않고 그중에서 sampling해서 뽑아서 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- word to vec의 단점을 파헤침\n",
    "- global한 occurance정보가 맞게 embedding을 하자?\n",
    "- 벡터들의 내적이 8.9가 되도록 ?\n",
    "- 처음에 시작하기전에 Co occurance list를 먼저 만들고 시작하자\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 단어 내적인 정보도 사용하자 (love, loves, loved 뭐 이런거..?)\n",
    "- love 는 l, o, v, e 의 chracter의 back of word이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
