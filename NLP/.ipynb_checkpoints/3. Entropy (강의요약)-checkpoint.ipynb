{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 3강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 어떻게 데이터를 압축시켜보내는게 데이터를 최대한 보낼수 있을까? (엔트로피)\n",
    "- 통신에서 많이 쓰이는 단어는 적은 비트로 / 희귀한것들은 긴 비트로 코딩해서 전송"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 엔트로피는 평균적으로 기대할수 있는 정보량임\n",
    "- 일반적인 사건의 기댓값과 마찬가지로 확률 * 정보량\n",
    "- 엔트로피값은 평균적인 불확실성을 보여준다\n",
    "- 즉-> 엔트로피 결과값이 크면 불확실성이 높다\n",
    "- 불확실성이 가장 높은 경우는 모든 확률이 1/N 똑같을때? (Equal Probability)\n",
    "- 어느한쪽으로 Bias되어있으면 오히려 우리가 결고값을 추측하기가 쉬워진다.\n",
    "- 모든 확률이 Fair하면 오히려 정보의 불확실성이생긴다 - > 어떤결과가 일어날지 모른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 나중에 Language모델에 엔트로피를 적용 - > Test데이터에 테스트해보고\n",
    "- 만약 제대로 Fit되어잇으면 엔트로피 값이 작겠지?\n",
    "- 이걸로 나중에 딥런링등의 모델을 더블엔트로피 체크등으로 확인한다\n",
    "- 내가 과연 모델링을 잘 한것인지?\n",
    "- 정보량이 많으면 많을수록 해당 사건에 얻게되는 불확실성이 높다\n",
    "- 정보량이 높다? :  해당 사건이 발생하면 얻게되는 데이터가 많다\n",
    "- 엔트로피가 높다는것은? : 우리가 결과를 예측하기 힘들다\n",
    "- 딥러닝에서 손실율 계산할떄도 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8-side 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 첫번쨰 예제는 모든 확률이 1/8로 Equal하다는 가정으로 푼것임 (3비트)\n",
    "- 1번 예제까지하고 3-2강으로 넘어감 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 의 엔트로피는? (3-2 / 4.56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 내가 만든 모델이 실제 한국어 혹은 영어에 잘 들어맞는가?\n",
    "- Sentence의 N으로 나누면은 해당 Sentence의 단어별 Entorpy값을 얻을 수 있다.\n",
    "- H(L): 무한대 언어의 엔트로피값 / 1/n을 무한대로 나눈다 -> 무한한 길이의 언어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# McMillan-Breiman theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 가정1: Stochastic process is to be staionary\n",
    "- 가정2: Independcy (Like Bigram Independency)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 전체 언어의 엔트로피를 구하여라? 그럼 그냥 Drop해버리셈\n",
    "- 그래서 그냥 특정 Language Model을 만들어서 걔내를 구하고 * 모델의 확률\n",
    "- 실제 값 p를 구하고싶은데 모를때 m모델을 이욯해서 구하는 entorpy는 4.62공식\n",
    "- H(p,m)\n",
    "- H(p) <= H(p,m) : 내가한 모델링으로 이정도의 크로스 엔트로피가 나왔다? 근데 \n",
    " 만약 다른사람이 모델링한것이 내 값보다 더작다? 그럼 그사람이 맞는거다   \n",
    "- ex) 내가만든거 2.02300 / 다른사람이 2.001 -> 오? 너가더 낮네 이걸로 개선\n",
    "- 여러개의 H(P,M)을 이용해서 H(p)에 접근하는과정 == Cross Entropy Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 요즘 엔트로피를 대체하고 있는 개념\n",
    "- 엔트로피의 개선이 눈에보이지않을만큼 너무작다! 이걸 뻥튀기하자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2강 PPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 39p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navice Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- unkown words는 전체를 묶어서 unkown이라는 class로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- P(d)를 모두 무시할 수 있는 이유가 / P(d)는 모든 Document 는 같으니깐 그냥 날린다\n",
    "- Class: 정치 / D: document문서\n",
    "- 모든 class에 대해서 Navie Bayes를 구하고 거기에서 -> 값이 제일 큰것이 \n",
    "- 해당 Document의 클래스이다.\n",
    "- 이전 N-gram과 동일하게 0이되는 확률이 생길 수 있다.\n",
    "- 이것도 Laplace Smooting하면됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- China 클래스에서 Shanghai가 Generative할 경우가 얼마가 되느냐?\n",
    "- 클래스가 주어질때 특정 언어가 생성될 확률?\n",
    "- 41p \n",
    "- positive class면 = >  해당 확률들이고\n",
    "- 만약 pos일때 위의 i love this fun film이라는 sentence가 나올 확률은 해당 확률들의 곱이다.\n",
    "- 그럼 pos인가 neg인가를 계산해서 큰값으로 해당 문장이 분류된다\n",
    "- 예제는 pos한 문장으로 분류됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 45p 계산예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-  Navie Bayes는 Independent Assumption에서만 사용가능하다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
