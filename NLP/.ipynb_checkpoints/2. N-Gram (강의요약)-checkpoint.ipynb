{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1교시\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 모든 Sentence의 맨앞- > Start of Sentence / End of Sentence\n",
    "- Token: 전체 단어 갯수\n",
    "- Type: Token에서 중복된 단어 제외\n",
    "- 단수 & 복수 : 의미는 같은데 형태가 다른거 -> Lemma로 인식 \n",
    "- LemmaTization을 써야된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 실제 할때는 Lemma보단 그냥 Token으로 사용 / Cats <> Cat 이런걸 그냥 각각 Token으로 인식\n",
    "- 대신에 Cats, \"Cats\" Cats* 이런것처럼 토큰에 붙어있는 더러운것들을 없애는 PreProcessing을 잘해야된다.\n",
    "- 없애지 않으면 위의 예제 3개를 각각 토큰으로 인식\n",
    "- 12p Language Modeling: 난 이수업이 참 ~~해 (이전의 Sequence를 보고 뒷단어를 예측) / N-gram 사용\n",
    "- W1 = 난 , W2 = 이수업이 --- Wn-1 참 // = > Wn = 지루해\n",
    "- Corpus : 말뭉치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Projects\\keras_talk\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Languae Modeling: Corpus들 을 이용해서 Language의 확률들을 구하는것\n",
    "- 해당 Sequence가 Corupus에서 얼마나 있는지가 -> Count(A)이다.\n",
    "- 해당 말뭉치 다음에 특정 단어가 나올확률 -> P(A) \n",
    "- 일반 Conditional Probablilty 보다 -> ChainRule을 쓰자\n",
    "- Chain Rule: Sentence의 전체 단어를 쪼개서 Chain Rule로 만들어 쓰지않으면, 길이가 긴 Sentence가 \n",
    "완벽하게 나타날 확률은 너무 적다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 근데 이렇게 해도 Chain Rule에서는 체인이 길어질수록 뒤에 Condition Sentence가 너무 길어진다\n",
    "- >> 이걸 해결하고자 Independt Assumpiton을 가정해서 문제를 푼다. (Earlier History에 Independent 하다)\n",
    "- Markov Assumption: 여기에서 N은 본인이 선택한다 -> Bigram으로 할거면 N = 2 (앞의 두개만 본다)\n",
    "- Count(새빨간 거짓말) / Count(새빨간) !!(Unigram)\n",
    "- Corpus(참고 문서)가 달라지면 확률은 당연히 달라진다.\n",
    "- 전체 bigram에서 i' am이 나올확률보다 i가나오고 am이나올확률이 더크다 (그래서 이렇게 만드는게 Maximum Likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2교시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- UniGram 카운트는 그냥 P(A) // 바이그램은 P(A|B)\n",
    "- 30p -> 총 결과를 보여준다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30P ->>> 43으로 건너뜀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Zero Count: 마약에 Bigram으로 Chain을 만들어서 Probability를 구하는데 중간의 P가 확률이 0이면\n",
    "전체가 0이되어버리는데 이것을 어떻게 할 것이냐?\n",
    "- Laplace Smooting: 0인것을 0이 아닌걸로 만들어보자\n",
    "- 그냥 모든 Count를 1씩 증가시키자! 그럼 0은 안되겟지? // V:는 Vocabulary의 수 (모든걸 1씩 증가시키니깐 1*V을 분모에 더한다)\n",
    "- V: 비어있는 Count들의 수     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Reconstituted Count: 라플라스를 거친후 Count들이 얼마나 Adjust가 되었는지 확인\n",
    "- 전체 N이 얼마고 비어있는게 얼마있는지에 따라서 Original Count의 급격한 변이를 가져오게 된다.\n",
    "- 따라서 Simple하게 Add 1을 하는것은 사이즈가 큰 Corpus에서는 왜곡이 있을 때가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Good Turing을 사용하자 (Lapalce 대신에)\n",
    "- 지금까지 한번잡힌 것들로 아직 안잡힌 것들을 Estimation 하자?\n",
    "- N1 => 한번 잡힌 것들의 수\n",
    "- N10 -> 10번 잡힌 것들의 수\n",
    "- 한번출연한것은 또 출연하면 두번쨰가 되고 - > 세번 - > 네번?\n",
    "- ex) Nc = 1번 잡힌것 Nc+1  = 2번 잡힌것\n",
    "- 전체 비율을 Frequency of Frequency를 바탕으로 Adjust한 Good Turing을 사용하면\n",
    "-> 단순히 Add 1 했던 Laplace보다 전체 비율의 급격한 변화가 없음을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 66p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "from nltk.tokenize import *\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_dict(gram_list):\n",
    "    d = defaultdict(lambda: 0)\n",
    "    for gram in gram_list:\n",
    "        d[gram] += 1\n",
    "    return d        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Dict는 저장되지않은 key에 대해서는 Lambda값을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- zip은 Bigram을 만들기위한 방법임"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
