{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 회귀선은 전체적으로 산포도에 대한 예측오차(Y-Y*)이 최소가 되는 방법으로 그려진다.\n",
    "- 회귀식을 구하기위해서 s^2을먼저구하고\n",
    "- 해당 식을 X와Y로 편미분하면 구할 수 있다.\n",
    "- (x-x^)(y-y^) 는 x가 변할때 y값이 변하는 정도 -> Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Logistic Regression은 Linear Rgeression에서 각각의 변수들을 Vector로 날린거다\n",
    "- 벡터의 내적  \n",
    "- y = w-f (내적) => Logistic Regression\n",
    "- 반응변수의 값이 0 과 1로 binary 채워지는 것들이 Logistic Regression이다\n",
    "- Logistic Regression의 결과값은 0하고 1사이의 확률값으로 계산된다 (Linear Regression정의로 풀면)\n",
    "- ln(p/(1-p))의 승산확률이 구해진다 (23p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 로지스틱 function은 모든 Regression의 결과값을 0하고 1사이로 fitting시킨다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- test 데이터의 y값을 없애서 -> 실제 예측치를 비교해보려고\n",
    "- 마이너스 weigth를 positive 로 변경시키기위해서 log regression function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Maxent Model and Discriminateive Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 은행 vs 은행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 어떤 문제를 푸는데 특정 feature가 class를 결정하는데 결정적이다! 그럼 - > weight를 높게준다\n",
    "- 딥러닝을 통해 weight하고 feature를 자기가 알아서 찾아준다\n",
    "- 예제문제에서 f1 = 1.8/  f2 = -0.6 / f3 = 0.3 / f4(person) =  0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 딥러닝의 대부분은 마지막에 모든 결과를 확률로 바꿔줄때 soft max function을 사용한다.\n",
    "- Feature-based Linear Classfier 은 maxmize the conditional liklihood이다.\n",
    "- 24페이지 시험! (Quiz Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 34p 중요! / Hong / Kong double count\n",
    "- 실제는 asia 와 유럽이 eqaul probablilty인데 double counting때문에 asia로 잘못 예측하게됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 42p 엔트로피가 최대인곳을 구해서 -> 점점 낮춰가는곳을 찾아라!\n",
    "- unique한 엔트로피는 엔트로피가 최대! 즉 uniform한 분포를 가진다\n",
    "- 처음에 맥시멈 엔트로피를 가정하고 거기에서 시작하라\n",
    "- 그걸 깨지는 feature들을 넣을떄마다 갱신을해라\n",
    "- 해당 분포에서는 '배불뚝이 동전' 어떠한 제약사항도 할당하지 않았을때이다\n",
    "- feature를 새로 넣는순간 (in 다음에 대문자면 Lociation이다 이런거) -> 엔트로피는 낮아진다.\n",
    "- 반대로 MLE(Maximum likelihood)는 높아진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 하나의 특징을 찾았을때 해당 특징에 속하는 event들은 모두 다시 uniform하게 가정한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 마지막쪽 -> 확률은 double counting은 하지않지만\n",
    "-> weight값은 조절되게된다(줄어든다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 33p 더블 카운팅의 예시 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
