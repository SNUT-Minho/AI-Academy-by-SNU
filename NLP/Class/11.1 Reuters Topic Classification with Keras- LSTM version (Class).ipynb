{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Reuters Topic Classification with Keras- LSTM version\n",
    "TA: Bokyung Son (*Computational Linguistics Lab*)\n",
    "\n",
    "In this lab, we train several RNN variants on the Reuters Newswire topic classification task.\n",
    "1. Vanilla 1-layer LSTM\n",
    "2. Stacked LSTM\n",
    "3. Bidirectional LSTM\n",
    "4. Stacked bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, Dense, LSTM, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000  # use top `max_features` most common words\n",
    "maxlen = 80  # trim after `maxlen` number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_features, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982 train sequences\n",
      "2246 test sequences\n",
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "# check the structure\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 classes\n"
     ]
    }
   ],
   "source": [
    "# Check labels: 0~45\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# `pad_sequences` returns a 2d numpy array (num_samples, num_timesteps) \n",
    "# `num_timesteps` equals `maxlen`\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize labels\n",
    "print('Convert class vector to binary class matrix (for categorical_crossentropy)')\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS=30\n",
    "BATCH_SIZE=128\n",
    "EMBEDDING_DIM=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['mlp', 'vanilla', 'stacked', 'bi']\n",
    "\n",
    "BASE_PATH = './reuters_model/'\n",
    "if not os.path.exists(BASE_PATH):\n",
    "    os.mkdir(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model checkpoint callback\n",
    "def create_checkpoint(model_name):\n",
    "    # creates a subdirectory under `BASE_PATH`\n",
    "    MODEL_PATH = os.path.join(BASE_PATH, model_name)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.mkdir(MODEL_PATH)\n",
    "    \n",
    "    return ModelCheckpoint(filepath=os.path.join(MODEL_PATH, '{epoch:02d}-{val_loss:.4f}.hdf5'),\n",
    "                           monitor='val_loss',\n",
    "                           verbose=1,\n",
    "                           save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train(history):\n",
    "    fig, loss_ax = plt.subplots(figsize=(6,6))\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "    loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "    loss_ax.set_ylim([0.0, 3.0])\n",
    "\n",
    "    acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "    acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "    acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    acc_ax.set_ylabel('accuray')\n",
    "\n",
    "    loss_ax.legend(loc='upper left')\n",
    "    acc_ax.legend(loc='lower left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vanilla 1-layer LSTM\n",
    "See [documentation](https://keras.io/layers/recurrent/#lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, EMBEDDING_DIM, input_length=maxlen)) # output: (batch_size, timesteps, input_dim)\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) # input: (batch_size, timesteps, input_dim)\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 2. Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 3. Train model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=[early_stopping, create_checkpoint('vanilla')])\n",
    "\n",
    "plot_train(history)\n",
    "\n",
    "# 4. Evaluate model\n",
    "loss, acc = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print('----- Evaluation loss and metrics -----')\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, EMBEDDING_DIM, input_length=maxlen))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 2. Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 3. Train model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=[early_stopping, create_checkpoint('stacked')])\n",
    "\n",
    "plot_train(history)\n",
    "    \n",
    "# 4. Evaluate model\n",
    "loss, acc = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print('----- Evaluation loss and metrics -----')\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bidirectional LSTM\n",
    "See [documentation](https://keras.io/layers/wrappers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, EMBEDDING_DIM, input_length=maxlen))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# 앞으로보는놈 뒤로보는놈 \n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "# 2. Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 3. Train model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=MAX_EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=[early_stopping, create_checkpoint('bi')])\n",
    "\n",
    "plot_train(history)\n",
    "\n",
    "# 4. Evaluate model\n",
    "loss, acc = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "print('----- Evaluation loss and metrics -----')\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stacked bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: YOUR CODE HERE\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
