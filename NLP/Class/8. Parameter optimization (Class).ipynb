{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모멘텀 -> 기울기 학습량에 가속도를 가중한다?\n",
    "- x는 상대적으로 완만한데 -> y는 널뛰기 하듯이 움직인다 (비효율/ 가지 않아도 되는 곳을 방문함)\n",
    "- 각 방법이 데이터셋에 따라서 어떤게 좋을지 모르니 -> 다 해보고 찍어본다음에 고르자\n",
    "- 처음 w를 generate할때는 0.01의 표준편차를 가지는 것에서 random number를 뽑아서 사용해라\n",
    "- 60/20/20 으로 나눈다 (Train/ Parameter Optimization/ Test) 이렇게 데이터 셋을 나눈다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 벡터로 의미를 표시 -> 벡터끼리의 비슷한정도를 계산 distribution비교\n",
    "- 단어는 그 단어 주위를 보고 알 수 있다.\n",
    "- 단어를 벡터 공간상으로 embedding한다\n",
    "- Row 보는것은 해당 단어들의 유사성을 보는것이고\n",
    "- Vertical 하게 보는것은 각각의 Document간의 유사성을 보는 것이다.\n",
    "- 해당단어의 전후좌우로 봄\n",
    "- Context window 사이즈를 정하는 것이 중요해보인다\n",
    "- 주변 단어의 발생 빈도를 바탕으로 유사도를 측정 (Co-occurrence)\n",
    "- syntagmatic association: wrote -> book \n",
    "- paradigmatic association: wrote / said / remakred  -> book 앞의 세가지는 유사도가 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointwise Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두단어가 같이나올 확률을 -> 각각 나올 확률로 곱해서 나눈다 (새빨간,거짓말 -> 새빨간 거짓말)\n",
    "- 단순히 row count말고 이걸쓰자\n",
    "- 두단어 / 두 context가 얼마나 유사한지를 아는데 사용\n",
    "- 22p에서 왼쪽은 단어 위쪽은 context\n",
    "- fij에서 i는 단어 j는 context / 즉 i단어가 j context에서 나타날 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 24p 뭐가 이상하지않아? (교수님 질문) -> 이걸 물어보고 row count 페이지를 보러감\n",
    "- 그래서 Mutual information을 쓸때 조심해야될 상황 (row count에서 수치가 작은애들이 왜 확률이 높을까?)\n",
    "  1. x하고 y가 완전히 독립적일때 -> 그럼 확률이 pmmi 확률이무조건 1이됨 (p(x)*p(y)/p(x)*p(y))\n",
    "  2. x하고 y가 완전히 dependent 할때 => 1/p(y)가 된다 즉 y의 빈도가 작으면 작을수록 값이 커져버린다\n",
    "- 그럼 이것을 어떻게 개선할거냐? -> Weighting PMI\n",
    "- 25P부터 해결방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 의미의 유사성을 비교할때 코사인 similiarty를 많이 사용한다\n",
    "- 32p 단어 frequency에 영향을 받지않고 distance를 구하기 위해서는 vector를 normalize해야된다.\n",
    "(예시: banana 2, apple 2 / banana 4, apple 4 는 제일 유사한데-> distance를 그냥 구해버리면 거리가 가장 먼걸로 측정된다)\n",
    "- 36p에 consine(apricot 분모에 2가 아니라 4임)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
